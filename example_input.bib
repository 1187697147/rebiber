@article{lin2020birds,
	title={Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},
	author={Lin, Bill Yuchen and Lee, Seyeon and Khanna, Rahul and Ren, Xiang},
	journal={arXiv preprint arXiv:2005.00683},
	year={2020}
}

@inproceedings{
	Clark2020ELECTRA,
	title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
	author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
	booktitle={International Conference on Learning Representations},
	year={2020},
	
}

@inproceedings{
	Lan2020ALBERT,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	booktitle={International Conference on Learning Representations},
	year={2020},
	
}

@inproceedings{snli:emnlp2015,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}



@InProceedings{Wang_2019_ICCV,
	title={VATEX: A large-scale, high-quality multilingual dataset for video-and-language research},
	author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
	booktitle={Proceedings of the IEEE International Conference on Computer Vision},
	pages={4581--4591},
	year={2019},
	
}


@inproceedings{Annervaz2018LearningBD,
	title={Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing},
	author={K. M. Annervaz and Somnath Basu Roy Chowdhury and Ambedkar Dukkipati},
	booktitle={NAACL-HLT},
	year={2018}
}
@article{johnson1980mental,
	title={Mental models in cognitive science},
	author={Johnson-Laird, Philip N},
	journal={Cognitive science},
	volume={4},
	number={1},
	pages={71--115},
	year={1980},
	publisher={Elsevier}
} 
@inproceedings{Hudson2018CompositionalAN,
	title={Compositional Attention Networks for Machine Reasoning},
	author={Drew A. Hudson and Christopher D. Manning},
	booktitle={ICLR},
	year={2018}
}
@inproceedings{kipf2016semi,
	title={Semi-Supervised Classification with Graph Convolutional Networks},
	author={Kipf, Thomas N and Welling, Max},
	booktitle={Proceedings of ICLR},
	year={2017}
}
@inproceedings{Wang2019ImprovingNL,
	title={Improving Natural Language Inference Using External Knowledge in the Science Questions Domain},
	author={Xiaoyan Wang and Pavan Kapanipathi and Ryan Musa and Mo Yu and Kartik Talamadupula and Ibrahim Abdelaziz and Maria Chang and Achille Fokoue and Bassem Makni and Nicholas Mattei and Michael Witbrock},
	booktitle={AAAI},
	year={2019}
}


@inproceedings{puduppully-etal-2017-transition,
	title = "Transition-Based Deep Input Linearization",
	author = "Puduppully, Ratish  and
	Zhang, Yue  and
	Shrivastava, Manish",
	booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
	month = apr,
	year = "2017",
	address = "Valencia, Spain",
	publisher = "Association for Computational Linguistics",
	
	pages = "643--654",
	abstract = "Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules cannot be leveraged by all modules. We construct a transition-based model to jointly perform linearization, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our system achieves the best results reported so far.",
}

@inproceedings{wang-etal-2020-semeval,
	title = "{S}em{E}val-2020 Task 4: Commonsense Validation and Explanation",
	author = "Wang, Cunxiang  and
	Liang, Shuailong  and
	Jin, Yili  and
	Wang, Yilong  and
	Zhu, Xiaodan  and
	Zhang, Yue",
	booktitle = "Proceedings of The 14th International Workshop on Semantic Evaluation",
	year = "2020",
	publisher = "Association for Computational Linguistics",
	
}

@inproceedings{wang-etal-2019-make,
	title = "Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation",
	author = "Wang, Cunxiang  and
	Liang, Shuailong  and
	Zhang, Yue  and
	Li, Xiaonan  and
	Gao, Tian",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "4020--4026",
	abstract = "Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.",
}

@article{Yang2019XLNetGA,
	title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
	journal={ArXiv},
	year={2019},
	volume={abs/1906.08237}
}

@article{Wolf2019HuggingFacesTS,
	title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
	author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
	journal={ArXiv},
	year={2019},
	volume={abs/1910.03771},
	
}

@inproceedings{post-vilar-2018-fast,
	title = "Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation",
	author = "Post, Matt  and
	Vilar, David",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "1314--1324",
}


@inproceedings{Zhang2020BERTScore,
	title={"BERTScore: Evaluating Text Generation with BERT"},
	author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
	booktitle={International Conference on Learning Representations},
	year={2020},
	
}

@inproceedings{Rajani2019ExplainYL,
	title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
	author={Nazneen Fatema Rajani and Bryan McCann and Caiming Xiong and Richard Socher},
	booktitle={ACL},
	year={2019}
}

@inproceedings{Schlichtkrull2018ModelingRD,
	title={Modeling Relational Data with Graph Convolutional Networks},
	author={Michael Sejr Schlichtkrull and Thomas N. Kipf and Peter Bloem and Rianne van den Berg and Ivan Titov and Max Welling},
	booktitle={European Semantic Web Conference},
	year={2018}
}
@article{Hochreiter1997LongSM,
	title={Long Short-Term Memory},
	author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
	journal={Neural Computation},
	year={1997},
	volume={9},
	pages={1735-1780}
}
@inproceedings{Mihaylov2018KnowledgeableRE,
	title={Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge},
	author={Todor Mihaylov and Anette Frank},
	booktitle={ACL},
	year={2018}
}

@inproceedings{Yang2017LeveragingKB,
	title={Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
	author={Bishan Yang and Tom Michael Mitchell},
	booktitle={ACL},
	year={2017}
}

@inproceedings{Talmor2018CommonsenseQAAQ,
	title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
	author = "Talmor, Alon  and
	Herzig, Jonathan  and
	Lourie, Nicholas  and
	Berant, Jonathan",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "4149--4158",
	abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@inproceedings{sap-etal-2019-social,
	title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
	author = "Sap, Maarten  and
	Rashkin, Hannah  and
	Chen, Derek  and
	Le Bras, Ronan  and
	Choi, Yejin",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "4463--4473",
	abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@inproceedings{Fan2018HierarchicalNS,
	title = "Hierarchical Neural Story Generation",
	author = "Fan, Angela  and
	Lewis, Mike  and
	Dauphin, Yann",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "889--898",
	abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@inproceedings{Zellers2019FromRT,
	title={From recognition to cognition: Visual commonsense reasoning},
	author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={6720--6731},
	year={2019},
	
}

@inproceedings{Yang2019KnowledgeableSA,
	title={Knowledgeable storyteller: a commonsense-driven generative model for visual storytelling},
	author={Yang, Pengcheng and Luo, Fuli and Chen, Peng and Li, Lei and Yin, Zhiyi and He, Xiaodong and Sun, Xu},
	booktitle={Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI},
	pages={5356--5362},
	year={2019},
	
}

@inproceedings{LaiXLYH17,
	author    = {Guokun Lai and
	Qizhe Xie and
	Hanxiao Liu and
	Yiming Yang and
	Eduard H. Hovy},
	title     = {{RACE:} Large-scale ReAding Comprehension Dataset From Examinations},
	booktitle={EMNLP},
	year={2017}
}

@inproceedings{yang2018hotpotqa,
	title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
	author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	booktitle={EMNLP},
	year={2018}
}

@inproceedings{Zellers2018SWAGAL,
	title = "{SWAG}: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
	author = "Zellers, Rowan  and
	Bisk, Yonatan  and
	Schwartz, Roy  and
	Choi, Yejin",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "93--104",
	abstract = "Given a partial description like {``}she opened the hood of the car,{''} humans can reason about the situation and anticipate what might come next ({''}then, she examined the engine{''}). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88{\%}), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
}

@inproceedings{Pascanu2014HowTC,
	title={How to Construct Deep Recurrent Neural Networks},
	author={Razvan Pascanu and Çaglar G{\"u}lçehre and Kyunghyun Cho and Yoshua Bengio},
	journal={ICLR},
	year={2014}, 
}  
@inproceedings{Dong2019UnifiedLM,
	title={Unified language model pre-training for natural language understanding and generation},
	author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
	booktitle={Advances in Neural Information Processing Systems},
	pages={13042--13054},
	year={2019},
	
}

@inproceedings{Lin2004ROUGEAP,
	title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
	author = "Lin, Chin-Yew",
	booktitle = "Text Summarization Branches Out",
	month = jul,
	year = "2004",
	address = "Barcelona, Spain",
	publisher = "Association for Computational Linguistics",
	
	pages = "74--81",
}

@inproceedings{Anderson2016SPICESP,
	title={Spice: Semantic propositional image caption evaluation},
	author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	booktitle={European Conference on Computer Vision},
	pages={382--398},
	year={2016},
	organization={Springer},
	
}

@inproceedings{Vedantam2014CIDErCI,
	title={Cider: Consensus-based image description evaluation},
	author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4566--4575}, 
	year={2015},
	
}

@inproceedings{Papineni2001BleuAM,
	title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
	author = "Papineni, Kishore  and
	Roukos, Salim  and
	Ward, Todd  and
	Zhu, Wei-Jing",
	booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2002",
	address = "Philadelphia, Pennsylvania, USA",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "311--318",
}



@inproceedings{Miao2018CGMHCS,
	title={Cgmh: Constrained sentence generation by metropolis-hastings sampling},
	author={Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={33},
	pages={6834--6842},
	year={2019},
	
}

@inproceedings{See2017GetTT,
	title={Get To The Point: Summarization with Pointer-Generator Networks},
	author={Abigail See and Peter J. Liu and Christopher D. Manning},
	booktitle={ACL},
	year={2017}
}


@book{arbib1987schema,
	title={From schema theory to language.},
	author={Arbib, Michael A and Conklin, E Jeffrey and Hill, Jane C},
	year={1987},
	publisher={Oxford University Press}
}

@article{axelrod1973schema,
	title={Schema theory: An information processing model of perception and cognition},
	author={Axelrod, Robert},
	journal={American political science review},
	volume={67},
	number={4},
	pages={1248--1266},
	year={1973},
	publisher={Cambridge University Press}
}

@article{arbib1992schema,
	title={Schema theory},
	author={Arbib, Michael A},
	journal={The Encyclopedia of Artificial Intelligence},
	volume={2},
	pages={1427--1443},
	year={1992},
	publisher={Wiley-Interscience}
}

@article{anderson1984schema,
	title={A schema-theoretic view of basic processes in reading comprehension},
	author={Anderson, Richard C and Pearson, P David},
	journal={Handbook of reading research},
	volume={1},
	pages={255--291},
	year={1984}
}



@inproceedings{DBLP:conf/cvpr/LuYBP18,
	author    = {Jiasen Lu and
	Jianwei Yang and
	Dhruv Batra and
	Devi Parikh},
	title     = {Neural Baby Talk},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
	{CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
	pages     = {7219--7228},
	publisher = {{IEEE} Computer Society},
	year      = {2018},
	
	doi       = {10.1109/CVPR.2018.00754},
	timestamp = {Wed, 16 Oct 2019 14:14:50 +0200}, 
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{j-kurisinkel-chen-2019-set,
	title = "Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes",
	author = "J Kurisinkel, Litton  and
	Chen, Nancy",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "6165--6175",
	abstract = "We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of Diseases) codes. This task differs from other natural language generation tasks in the following ways: (1) The input is a set of identifiable entities (ICD codes) where the relations between individual entity are not explicitly specified. (2) The output text is not a narrative description (e.g. news articles) composed from the input. Rather, inferences are made from the input (symptoms specified in ICD codes) to generate the output (instructions). (3) There is an optimal order in which each sentence (instruction) should appear in the output. Unlike most other tasks, neither the input (ICD codes) nor their corresponding symptoms appear in the output, so the ordering of the output instructions needs to be learned in an unsupervised fashion. Based on clinical intuition, we hypothesize that each instruction in the output is mapped to a subset of ICD codes specified in the input. We propose a neural architecture that jointly models (a) subset selection: choosing relevant subsets from a set of input entities; (b) content ordering: learning the order of instructions; and (c) text generation: representing the instructions corresponding to the selected subsets in natural language. In addition, we penalize redundancy during beam search to improve tractability for long text generation. Our model outperforms baseline models in BLEU scores and human evaluation. We plan to extend this work to other tasks such as recipe generation from ingredients.",
}
@inproceedings{Devlin2019,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={NAACL-HLT},
	year={2019}
}

@inproceedings{Zhang2019ConversationGW,
	title={Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs},
	author={Houyu Zhang and Zhenghao Liu and Chenyan Xiong and Zhiyuan Liu},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	note={to appear},
	year={2020}, 
	
}



@inproceedings{Susanto2020LexicallyCN,
	title={Lexically Constrained Neural Machine Translation with Levenshtein Transformer},
	author={Raymond Hendy Susanto and Shamil Chollampatt and Li-ling Tan},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	note={to appear},
	year={2020},
	
}


@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}

@misc{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019},
	
}

@article{radford2018improving,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}, 
	year={2018}
}

@article{zhou2018graph,
	title={Graph Neural Networks: A Review of Methods and Applications},
	author={Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
	journal={arXiv preprint arXiv:1812.08434},
	year={2018}
}

@article{jaume2018image,
	title={Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks},
	author={Jaume, Guillaume and Bozorgtabar, Behzad and Ekenel, Hazim Kemal and Thiran, Jean-Philippe and Gabrani, Maria},
	journal={arXiv preprint arXiv:1811.03830},
	year={2018}
}

@article{Wang2018ImprovingNL,
	title={Improving Natural Language Inference Using External Knowledge in the Science Questions Domain},
	author={Xiaoyan Wang and Pavan Kapanipathi and Ryan Musa and Mo Yu and Kartik Talamadupula and Ibrahim Abdelaziz and Maria Chang and Achille Fokoue and Bassem Makni and Nicholas Mattei and Michael Witbrock},
	booktitle={AAAI},
	year={2019}
}

@inproceedings{Khashabi2017LearningWI,
	title={Learning What is Essential in Questions},
	author={Daniel Khashabi and Tushar Khot and Ashutosh Sabharwal and Dan Roth},
	booktitle={CoNLL},
	year={2017}
}


@article{garey1977rectilinear,
	title={The rectilinear Steiner tree problem is NP-complete},
	author={Garey, Michael R and Johnson, David S.},
	journal={SIAM Journal on Applied Mathematics},
	volume={32},
	number={4},
	pages={826--834},
	year={1977},
	publisher={SIAM}
}


@article{Zhong2018ImprovingQA,
	title={Improving Question Answering by Commonsense-Based Pre-Training},
	author={Wanjun Zhong and Duyu Tang and Nan Duan and Ming Zhou and Jiahai Wang and Jian Yin},
	journal={ArXiv},
	year={2018},
	volume={abs/1809.03568}
}

@inproceedings{Wang2014KnowledgeGE,
	title={Knowledge Graph Embedding by Translating on Hyperplanes},
	author={Zhen Wang and Jianwen Zhang and Jianlin Feng and Zheng Chen},
	booktitle={AAAI},
	year={2014}
}

@inproceedings{Singh2018SemanticallyEA,
	title={Semantically Equivalent Adversarial Rules for Debugging NLP models},
	author={Sameer Singh and Carlos Guestrin and Marco T{\'u}lio Ribeiro},
	booktitle={ACL},
	year={2018}
}

@article{weissenborn2017dynamic,
	title={Dynamic integration of background knowledge in neural NLU systems},
	author={Weissenborn, Dirk and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Dyer, Chris},
	journal={arXiv preprint arXiv:1706.02596},
	year={2017}
} 

@inproceedings{Kingma2015AdamAM,
	title={Adam: A Method for Stochastic Optimization},
	author={Diederik P. Kingma and Jimmy Ba},
	booktitle={ICLR},
	year={2015}, 
}

@article{Battaglia2018RelationalIB,
	title={Relational inductive biases, deep learning, and graph networks},
	author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vin{\'i}cius Flores Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çaglar G{\"u}lçehre and Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matthew Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
	journal={CoRR},
	year={2018},
	volume={abs/1806.01261}
}

@inproceedings{Yang2016HierarchicalAN,
	title={Hierarchical Attention Networks for Document Classification},
	author={Zichao Yang and Diyi Yang and Chris Dyer and Xiaodong He and Alexander J. Smola and Eduard H. Hovy},
	booktitle={NAACL-HLT},
	year={2016}
}

@article{das2019multi,
  title={Multi-step retriever-reader interaction for scalable open-domain question answering},
  author={Das, Rajarshi and Dhuliawala, Shehzaad and Zaheer, Manzil and McCallum, Andrew},
  journal={arXiv preprint arXiv:1905.05733},
  year={2019}
}

@article{lee2019latent,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1906.00300},
  year={2019}
}

@article{chen2017reading,
  title={Reading wikipedia to answer open-domain questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  journal={arXiv preprint arXiv:1704.00051},
  year={2017}
}

@inproceedings{li2016commonsense,
	title={Commonsense knowledge base completion},
	author={Li, Xiang and Taheri, Aynaz and Tu, Lifu and Gimpel, Kevin},
	booktitle={ACL}, 
	year={2016}
}

@article{qi2019answering,
  title={Answering complex open-domain questions through iterative query generation},
  author={Qi, Peng and Lin, Xiaowen and Mehr, Leo and Wang, Zijian and Manning, Christopher D},
  journal={arXiv preprint arXiv:1910.07000},
  year={2019}
}

@article{sun2018open,
  title={Open domain question answering using early fusion of knowledge bases and text},
  author={Sun, Haitian and Dhingra, Bhuwan and Zaheer, Manzil and Mazaitis, Kathryn and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:1809.00782},
  year={2018}
}

@inproceedings{feldman-el-yaniv-2019-multi,
    title = "Multi-Hop Paragraph Retrieval for Open-Domain Question Answering",
    author = "Feldman, Yair  and
      El-Yaniv, Ran",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "2296--2309",
    abstract = "This paper is concerned with the task of multi-hop open-domain Question Answering (QA). This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively.",
}

@article{sun2019pullnet,
  title={Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text},
  author={Sun, Haitian and Bedrax-Weiss, Tania and Cohen, William W},
  journal={arXiv preprint arXiv:1904.09537},
  year={2019}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={arXiv preprint arXiv:2005.11401},
  year={2020}
}

@article{raffel2019exploring,
	title={Exploring the limits of transfer learning with a unified text-to-text transformer},
	author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	journal={arXiv preprint arXiv:1910.10683},
	year={2019},
	
}
 


@inproceedings{Banerjee2005METEORAA,
	title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
	author = "Banerjee, Satanjeev  and
	Lavie, Alon",
	booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
	month = jun,
	year = "2005",
	address = "Ann Arbor, Michigan",
	publisher = "Association for Computational Linguistics",
	
	pages = "65--72",
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{lv2020graph,
  title={Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering.},
  author={Lv, Shangwen and Guo, Daya and Xu, Jingjing and Tang, Duyu and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Hu, Songlin},
  booktitle={AAAI},
  pages={8449--8456},
  year={2020}
}


@article{johnson2019billion,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  publisher={IEEE}
}

@inproceedings{Hasler2018NeuralMT,
	title = "Neural Machine Translation Decoding with Terminology Constraints",
	author = "Hasler, Eva  and
	de Gispert, Adri{\`a}  and
	Iglesias, Gonzalo  and
	Byrne, Bill",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "506--512",
	abstract = "Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.",
}
@inproceedings{Dinu2019TrainingNM,
	title = "Training Neural Machine Translation to Apply Terminology Constraints",
	author = "Dinu, Georgiana  and
	Mathur, Prashant  and
	Federico, Marcello  and
	Al-Onaizan, Yaser",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "3063--3068",
	abstract = "This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",
}

@inproceedings{Li2018DeleteRG,
	title = "Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",
	author = "Li, Juncen  and
	Jia, Robin  and
	He, He  and
	Liang, Percy",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "1865--1874",
	abstract = "We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., {``}screen is just the right size{''} to {``}screen is too small{''}). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., {``}too small{''}). Our strongest method extracts content words by deleting phrases associated with the sentence{'}s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22{\%} more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.",
}

@inproceedings{Lin2018MiningCD,
	title = "Mining Cross-Cultural Differences and Similarities in Social Media",
	author = "Lin, Bill Yuchen  and
	Xu, Frank F.  and
	Zhu, Kenny  and
	Hwang, Seung-won",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "709--719",
	abstract = "Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.",
}

@article{seo2019real,
  title={Real-time open-domain question answering with dense-sparse phrase index},
  author={Seo, Minjoon and Lee, Jinhyuk and Kwiatkowski, Tom and Parikh, Ankur P and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1906.05807},
  year={2019}
}

@article{cohen2019neural,
  title={Neural query language: A knowledge base query language for tensorflow},
  author={Cohen, William W and Siegler, Matthew and Hofer, Alex},
  journal={arXiv preprint arXiv:1905.06209},
  year={2019}
}

@inproceedings{
Cohen2020Scalable,
title={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},
author={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},
booktitle={International Conference on Learning Representations},
year={2020},

}

@article{Zhu2019TextI,
	title={Text Infilling},
	author={Wanrong Zhu and Zhiting Hu and Eric P. Xing},
	journal={ArXiv},
	year={2019},
	volume={abs/1901.00158},
	
}

@article{Luo2019ADR,
	title={A dual reinforcement learning framework for unsupervised text style transfer},
	author={Luo, Fuli and Li, Peng and Zhou, Jie and Yang, Pengcheng and Chang, Baobao and Sui, Zhifang and Sun, Xu},
	journal={arXiv preprint arXiv:1905.10060},
	year={2019}, 
	
}


@inproceedings{Luo2019TowardsFT,
	title = "Towards Fine-grained Text Sentiment Transfer",
	author = "Luo, Fuli  and
	Li, Peng  and
	Yang, Pengcheng  and
	Zhou, Jie  and
	Tan, Yutong  and
	Chang, Baobao  and
	Sui, Zhifang  and
	Sun, Xu",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "2013--2022",
	abstract = "In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation.",
}

@inproceedings{fu2018style,
	title={Style transfer in text: Exploration and evaluation},
	author={Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
	booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
	year={2018},
	
}

@inproceedings{Levesque2011TheWS,
	title={The Winograd Schema Challenge},
	author={Hector J. Levesque},
	booktitle={AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
	year={2011}
}


@inproceedings{sap2018atomic,
	title={Atomic: An atlas of machine commonsense for if-then reasoning},
	author={Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={33},
	pages={3027--3035},
	year={2019},
	
}

@inproceedings{Tandon2017WebChild2,
	title={WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation},
	author={Niket Tandon and Gerard de Melo and Gerhard Weikum},
	booktitle={ACL},
	year={2017}
}

@inproceedings{Santoro2017ASN,
	title={A simple neural network module for relational reasoning},
	author={Adam Santoro and David Raposo and David G. T. Barrett and Mateusz Malinowski and Razvan Pascanu and Peter W. Battaglia and Timothy P. Lillicrap},
	booktitle={NIPS},
	year={2017}
}


@inproceedings{Marcheggiani2017EncodingSW,
	title={Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling},
	author={Diego Marcheggiani and Ivan Titov},
	booktitle={EMNLP},
	year={2017}
}

@inproceedings{Bastings2017GraphCE,
	title={Graph Convolutional Encoders for Syntax-aware Neural Machine Translation},
	author={Joost Bastings and Ivan Titov and Wilker Aziz and Diego Marcheggiani and Khalil Sima'an},
	booktitle={EMNLP},
	year={2017}
}

@inproceedings{Zhang2018GraphCO,
	title={Graph Convolution over Pruned Dependency Trees Improves Relation Extraction},
	author={Yuhao Zhang and Peng Qi and Christopher D. Manning},
	booktitle={EMNLP},
	year={2018}
}

@book{Aho:72,
	author  = {Alfred V. Aho and Jeffrey D. Ullman},
	title   = {The Theory of Parsing, Translation and Compiling},
	year    = "1972",
	volume  = "1",
	publisher = {Prentice-Hall},
	address = {Englewood Cliffs, NJ}
}

@book{APA:83,
	author  = {{American Psychological Association}},
	title   = {Publications Manual},
	year    = "1983",
	publisher = {American Psychological Association},
	address = {Washington, DC}
}

@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512"
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	
}

@inproceedings{andrew2007scalable,
	title={Scalable training of {L1}-regularized log-linear models},
	author={Andrew, Galen and Gao, Jianfeng},
	booktitle={ICML},
	pages={33--40},
	year={2007},
}

@inproceedings{Zhou2017EmotionalCM,
	title={Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory},
	author={Hao Zhou and Minlie Huang and Tianyang Zhang and Xiaoyan Zhu and Bing Liu},
	booktitle={AAAI},
	year={2017}
}

@article{Qiao2019MirrorGANLT,
	title={MirrorGAN: Learning Text-to-image Generation by Redescription},
	author={Tingting Qiao and Jing Zhang and Duanqing Xu and Dacheng Tao},
	journal={ArXiv},
	year={2019},
	volume={abs/1903.05854}
}

@inproceedings{Hudson2019GQAAN,
	title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
	author={Drew A. Hudson and Christopher D. Manning},
	booktitle={CVPR},
	year={2019}
}

@inproceedings{Zhou2018CommonsenseKA,
	title={Commonsense Knowledge Aware Conversation Generation with Graph Attention},
	author={Hao Zhou and Tom Young and Minlie Huang and Haizhou Zhao and Jingfang Xu and Xiaoyan Zhu},
	booktitle={IJCAI},
	year={2018}
}

@inproceedings{Feng2018TopictoEssayGW,
	title={Topic-to-Essay Generation with Neural Networks.},
	author={Feng, Xiaocheng and Liu, Ming and Liu, Jiahao and Qin, Bing and Sun, Yibo and Liu, Ting},
	booktitle={IJCAI},
	pages={4078--4084},
	year={2018},
	
}

@inproceedings{Yang2019EnhancingTG,
	title = "Enhancing Topic-to-Essay Generation with External Commonsense Knowledge",
	author = "Yang, Pengcheng  and
	Li, Lei  and
	Luo, Fuli  and
	Liu, Tianyu  and
	Sun, Xu",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	
	
	pages = "2002--2012",
	abstract = "Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.",
}
@inproceedings{Guan2018StoryEG,
	title={Story ending generation with incremental encoding and commonsense knowledge},
	author={Guan, Jian and Wang, Yansen and Huang, Minlie},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={33},
	pages={6473--6480},
	year={2019},
	
}
@book{Gusfield:97,
	author  = {Dan Gusfield},
	title   = {Algorithms on Strings, Trees and Sequences},
	year    = "1997",
	publisher = {Cambridge University Press},
	address = {Cambridge, UK}
}

@inproceedings{borsch2011,
	Address = {Canberra, Australia},
	Author = {Benjamin Borschinger and Mark Johnson},
	Booktitle = {the Australasian Language Technology Association Workshop},
	Month = {December},
	Pages = {10--18},
	Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
	Year = {2011}}

@article{rasooli-tetrault-2015,
	author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
	title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
	journal   = {Computing Research Repository},
	volume    = {arXiv:1503.06733},
	year      = {2015},
	note    = {version 2}
}
@inproceedings{Weissenborn2018DynamicIO,
	title={Dynamic Integration of Background Knowledge in Neural NLU Systems},
	author={Dirk Weissenborn and Tom'avs Kovcisk'y and Chris Dyer},
	year={2018}
}

@inproceedings{bollacker2008freebase,
	title={Freebase: a collaboratively created graph database for structuring human knowledge},
	author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
	booktitle={SIGKDD},
	pages={1247--1250},
	year={2008},
	organization={AcM}
}
@article{shen2014entity,
	title={Entity linking with a knowledge base: Issues, techniques, and solutions},
	author={Shen, Wei and Wang, Jianyong and Han, Jiawei},
	journal={TKDE},
	volume={27},
	number={2},
	pages={443--460},
	year={2014},
	publisher={IEEE}
}

@article{Khashabi2019OnTC,
	title={On the Capabilities and Limitations of Reasoning for Natural Language Understanding},
	author={Daniel Khashabi and Erfan Sadeqi Azer and Tushar Khot and Ashutosh Sabharwal and Dan Roth},
	journal={CoRR},
	year={2019},
	volume={abs/1901.02522}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@InProceedings{P16-1001,
	author =  "Goodman, James
	and Vlachos, Andreas
	and Naradowsky, Jason",
	title =    "Noise reduction and targeted exploration in imitation learning for      Abstract Meaning Representation parsing    ",
	booktitle =       "ACL",
	year =    "2016",
	publisher = "Association for Computational Linguistics",
	pages =   "1--11",
	location =  "Berlin, Germany",
	doi =    "10.18653/v1/P16-1001",
}

@InProceedings{C14-1001,
	author =  "Harper, Mary",
	title =   "Learning from 26 Languages: Program Management and Science in the Babel Program",
	booktitle =   "COLING",
	year =    "2014",
	publisher = "Dublin City University and Association for Computational Linguistics",
	pages =   "1",
	location =  "Dublin, Ireland",
}

@article{Sakaguchi2019WINOGRANDEAA,
	title={WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale},
	author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
	journal={ArXiv},
	year={2019},
	volume={abs/1907.10641}
}

@article{susanto2020lexically,
	title={Lexically Constrained Neural Machine Translation with Levenshtein Transformer},
	author={Susanto, Raymond Hendy and Chollampatt, Shamil and Tan, Liling},
	journal={arXiv preprint arXiv:2004.12681},
	year={2020}
}
